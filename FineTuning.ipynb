{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-tuning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vocVQoW5QBKl",
        "outputId": "3f9ef906-f518-45e4-df33-93cc2f43123d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        }
      },
      "source": [
        "# Instalar la librería transformers de HuggingFace\n",
        "!pip3 install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting tokenizers==0.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 22.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 34.7MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 36.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=ab85ad7ebb778e6ef3300b1289e6be05e0b96f02bd66669293cae60481845f21\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.9.2 transformers-3.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWUr0QQTTFib",
        "outputId": "0c7f1d9c-e662-400e-aada-eac5e1e08fac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Ruta absoluta del fichero train.txt\n",
        "!readlink -f ./drive/My\\ Drive/discursos_reina/train.txt\n",
        "!ls ./drive/My\\ Drive/discursos_reina/train.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/discursos_reina/train.txt\n",
            "'./drive/My Drive/discursos_reina/train.txt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ppwg8ta8nqvl",
        "outputId": "f2860204-3b8f-4740-ab12-6da6e8d53548",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Conocer los argumentos\n",
        "!python3 ./drive/My\\ Drive/run_language_modeling.py --help"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-26 11:18:02.323630: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "usage: run_language_modeling.py [-h] [--model_name_or_path MODEL_NAME_OR_PATH]\n",
            "                                [--model_type MODEL_TYPE]\n",
            "                                [--config_name CONFIG_NAME]\n",
            "                                [--tokenizer_name TOKENIZER_NAME]\n",
            "                                [--cache_dir CACHE_DIR]\n",
            "                                [--train_data_file TRAIN_DATA_FILE]\n",
            "                                [--eval_data_file EVAL_DATA_FILE]\n",
            "                                [--line_by_line] [--mlm]\n",
            "                                [--mlm_probability MLM_PROBABILITY]\n",
            "                                [--plm_probability PLM_PROBABILITY]\n",
            "                                [--max_span_length MAX_SPAN_LENGTH]\n",
            "                                [--block_size BLOCK_SIZE] [--overwrite_cache]\n",
            "                                --output_dir OUTPUT_DIR\n",
            "                                [--overwrite_output_dir] [--do_train]\n",
            "                                [--do_eval] [--do_predict]\n",
            "                                [--evaluate_during_training]\n",
            "                                [--evaluation_strategy {EvaluationStrategy.NO,EvaluationStrategy.STEPS,EvaluationStrategy.EPOCH}]\n",
            "                                [--prediction_loss_only]\n",
            "                                [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
            "                                [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
            "                                [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
            "                                [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
            "                                [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "                                [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
            "                                [--learning_rate LEARNING_RATE]\n",
            "                                [--weight_decay WEIGHT_DECAY]\n",
            "                                [--adam_beta1 ADAM_BETA1]\n",
            "                                [--adam_beta2 ADAM_BETA2]\n",
            "                                [--adam_epsilon ADAM_EPSILON]\n",
            "                                [--max_grad_norm MAX_GRAD_NORM]\n",
            "                                [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "                                [--max_steps MAX_STEPS]\n",
            "                                [--warmup_steps WARMUP_STEPS]\n",
            "                                [--logging_dir LOGGING_DIR]\n",
            "                                [--logging_first_step]\n",
            "                                [--logging_steps LOGGING_STEPS]\n",
            "                                [--save_steps SAVE_STEPS]\n",
            "                                [--save_total_limit SAVE_TOTAL_LIMIT]\n",
            "                                [--no_cuda] [--seed SEED] [--fp16]\n",
            "                                [--fp16_opt_level FP16_OPT_LEVEL]\n",
            "                                [--local_rank LOCAL_RANK]\n",
            "                                [--tpu_num_cores TPU_NUM_CORES]\n",
            "                                [--tpu_metrics_debug] [--debug]\n",
            "                                [--dataloader_drop_last]\n",
            "                                [--eval_steps EVAL_STEPS]\n",
            "                                [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
            "                                [--past_index PAST_INDEX]\n",
            "                                [--run_name RUN_NAME]\n",
            "                                [--disable_tqdm DISABLE_TQDM]\n",
            "                                [--no-remove_unused_columns]\n",
            "                                [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
            "                                [--load_best_model_at_end]\n",
            "                                [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
            "                                [--greater_is_better GREATER_IS_BETTER]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --model_name_or_path MODEL_NAME_OR_PATH\n",
            "                        The model checkpoint for weights initialization. Leave\n",
            "                        None if you want to train a model from scratch.\n",
            "  --model_type MODEL_TYPE\n",
            "                        If training from scratch, pass a model type from the\n",
            "                        list: layoutlm, t5, distilbert, albert, camembert,\n",
            "                        xlm-roberta, marian, fsmt, bart, longformer, roberta,\n",
            "                        squeezebert, bert, openai-gpt, gpt2, mobilebert,\n",
            "                        transfo-xl, xlnet, flaubert, xlm, ctrl, electra,\n",
            "                        encoder_decoder, reformer, funnel\n",
            "  --config_name CONFIG_NAME\n",
            "                        Pretrained config name or path if not the same as\n",
            "                        model_name\n",
            "  --tokenizer_name TOKENIZER_NAME\n",
            "                        Pretrained tokenizer name or path if not the same as\n",
            "                        model_name\n",
            "  --cache_dir CACHE_DIR\n",
            "                        Where do you want to store the pretrained models\n",
            "                        downloaded from s3\n",
            "  --train_data_file TRAIN_DATA_FILE\n",
            "                        The input training data file (a text file).\n",
            "  --eval_data_file EVAL_DATA_FILE\n",
            "                        An optional input evaluation data file to evaluate the\n",
            "                        perplexity on (a text file).\n",
            "  --line_by_line        Whether distinct lines of text in the dataset are to\n",
            "                        be handled as distinct sequences.\n",
            "  --mlm                 Train with masked-language modeling loss instead of\n",
            "                        language modeling.\n",
            "  --mlm_probability MLM_PROBABILITY\n",
            "                        Ratio of tokens to mask for masked language modeling\n",
            "                        loss\n",
            "  --plm_probability PLM_PROBABILITY\n",
            "                        Ratio of length of a span of masked tokens to\n",
            "                        surrounding context length for permutation language\n",
            "                        modeling.\n",
            "  --max_span_length MAX_SPAN_LENGTH\n",
            "                        Maximum length of a span of masked tokens for\n",
            "                        permutation language modeling.\n",
            "  --block_size BLOCK_SIZE\n",
            "                        Optional input sequence length after tokenization.The\n",
            "                        training dataset will be truncated in block of this\n",
            "                        size for training.Default to the model max input\n",
            "                        length for single sentence inputs (take into account\n",
            "                        special tokens).\n",
            "  --overwrite_cache     Overwrite the cached training and evaluation sets\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        The output directory where the model predictions and\n",
            "                        checkpoints will be written.\n",
            "  --overwrite_output_dir\n",
            "                        Overwrite the content of the output directory.Use this\n",
            "                        to continue training if output_dir points to a\n",
            "                        checkpoint directory.\n",
            "  --do_train            Whether to run training.\n",
            "  --do_eval             Whether to run eval on the dev set.\n",
            "  --do_predict          Whether to run predictions on the test set.\n",
            "  --evaluate_during_training\n",
            "                        Run evaluation during training at each logging step.\n",
            "  --evaluation_strategy {EvaluationStrategy.NO,EvaluationStrategy.STEPS,EvaluationStrategy.EPOCH}\n",
            "                        Run evaluation during training at each logging step.\n",
            "  --prediction_loss_only\n",
            "                        When performing evaluation and predictions, only\n",
            "                        returns the loss.\n",
            "  --per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE\n",
            "                        Batch size per GPU/TPU core/CPU for training.\n",
            "  --per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE\n",
            "                        Batch size per GPU/TPU core/CPU for evaluation.\n",
            "  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\n",
            "                        Deprecated, the use of `--per_device_train_batch_size`\n",
            "                        is preferred. Batch size per GPU/TPU core/CPU for\n",
            "                        training.\n",
            "  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\n",
            "                        Deprecated, the use of `--per_device_eval_batch_size`\n",
            "                        is preferred.Batch size per GPU/TPU core/CPU for\n",
            "                        evaluation.\n",
            "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
            "                        Number of updates steps to accumulate before\n",
            "                        performing a backward/update pass.\n",
            "  --eval_accumulation_steps EVAL_ACCUMULATION_STEPS\n",
            "                        Number of predictions steps to accumulate before\n",
            "                        moving the tensors to the CPU.\n",
            "  --learning_rate LEARNING_RATE\n",
            "                        The initial learning rate for Adam.\n",
            "  --weight_decay WEIGHT_DECAY\n",
            "                        Weight decay if we apply some.\n",
            "  --adam_beta1 ADAM_BETA1\n",
            "                        Beta1 for Adam optimizer\n",
            "  --adam_beta2 ADAM_BETA2\n",
            "                        Beta2 for Adam optimizer\n",
            "  --adam_epsilon ADAM_EPSILON\n",
            "                        Epsilon for Adam optimizer.\n",
            "  --max_grad_norm MAX_GRAD_NORM\n",
            "                        Max gradient norm.\n",
            "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
            "                        Total number of training epochs to perform.\n",
            "  --max_steps MAX_STEPS\n",
            "                        If > 0: set total number of training steps to perform.\n",
            "                        Override num_train_epochs.\n",
            "  --warmup_steps WARMUP_STEPS\n",
            "                        Linear warmup over warmup_steps.\n",
            "  --logging_dir LOGGING_DIR\n",
            "                        Tensorboard log dir.\n",
            "  --logging_first_step  Log and eval the first global_step\n",
            "  --logging_steps LOGGING_STEPS\n",
            "                        Log every X updates steps.\n",
            "  --save_steps SAVE_STEPS\n",
            "                        Save checkpoint every X updates steps.\n",
            "  --save_total_limit SAVE_TOTAL_LIMIT\n",
            "                        Limit the total amount of checkpoints.Deletes the\n",
            "                        older checkpoints in the output_dir. Default is\n",
            "                        unlimited checkpoints\n",
            "  --no_cuda             Do not use CUDA even when it is available\n",
            "  --seed SEED           random seed for initialization\n",
            "  --fp16                Whether to use 16-bit (mixed) precision (through\n",
            "                        NVIDIA apex) instead of 32-bit\n",
            "  --fp16_opt_level FP16_OPT_LEVEL\n",
            "                        For fp16: Apex AMP optimization level selected in\n",
            "                        ['O0', 'O1', 'O2', and 'O3'].See details at\n",
            "                        https://nvidia.github.io/apex/amp.html\n",
            "  --local_rank LOCAL_RANK\n",
            "                        For distributed training: local_rank\n",
            "  --tpu_num_cores TPU_NUM_CORES\n",
            "                        TPU: Number of TPU cores (automatically passed by\n",
            "                        launcher script)\n",
            "  --tpu_metrics_debug   Deprecated, the use of `--debug` is preferred. TPU:\n",
            "                        Whether to print debug metrics\n",
            "  --debug               Whether to print debug metrics on TPU\n",
            "  --dataloader_drop_last\n",
            "                        Drop the last incomplete batch if it is not divisible\n",
            "                        by the batch size.\n",
            "  --eval_steps EVAL_STEPS\n",
            "                        Run an evaluation every X steps.\n",
            "  --dataloader_num_workers DATALOADER_NUM_WORKERS\n",
            "                        Number of subprocesses to use for data loading\n",
            "                        (PyTorch only). 0 means that the data will be loaded\n",
            "                        in the main process.\n",
            "  --past_index PAST_INDEX\n",
            "                        If >=0, uses the corresponding part of the output as\n",
            "                        the past state for next step.\n",
            "  --run_name RUN_NAME   An optional descriptor for the run. Notably used for\n",
            "                        wandb logging.\n",
            "  --disable_tqdm DISABLE_TQDM\n",
            "                        Whether or not to disable the tqdm progress bars.\n",
            "  --no-remove_unused_columns\n",
            "                        Remove columns not required by the model when using an\n",
            "                        nlp.Dataset.\n",
            "  --label_names LABEL_NAMES [LABEL_NAMES ...]\n",
            "                        The list of keys in your dictionary of inputs that\n",
            "                        correspond to the labels.\n",
            "  --load_best_model_at_end\n",
            "                        Whether or not to load the best model found during\n",
            "                        training at the end of training.\n",
            "  --metric_for_best_model METRIC_FOR_BEST_MODEL\n",
            "                        The metric to use to compare two different models.\n",
            "  --greater_is_better GREATER_IS_BETTER\n",
            "                        Whether the `metric_for_best_model` should be\n",
            "                        maximized or not.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf57nC_SQMrN",
        "outputId": "b405faec-b623-452c-ddf5-93a5ae5ae22d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Fine-tuning del modelo con nuestros datos\n",
        "!python3 ./drive/My\\ Drive/run_language_modeling.py \\\n",
        "  --output_dir='./drive/My Drive/output' \\\n",
        "  --overwrite_output_dir \\\n",
        "  --model_type=gpt2 \\\n",
        "  --model_name_or_path=gpt2 \\\n",
        "  --do_train \\\n",
        "  --train_data_file='./drive/My Drive/discursos_reina/train.txt' \\\n",
        "  --do_eval \\\n",
        "  --eval_data_file='./drive/My Drive/discursos_reina/eval.txt' \\\n",
        "  --per_device_train_batch_size=1 \\\n",
        "  --per_device_eval_batch_size=1 \\\n",
        "  --evaluate_during_training \\\n",
        "  --learning_rate=5e-5 \\\n",
        "  --num_train_epochs=3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-10 11:44:20.301525: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/training_args.py:332: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
            "  FutureWarning,\n",
            "10/10/2020 11:44:22 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "10/10/2020 11:44:22 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='./drive/My Drive/output', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Oct10_11-44-22_2acd5c319bd2', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=False)\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_auto.py:785: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1324: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
            "  FutureWarning,\n",
            "10/10/2020 11:44:28 - INFO - filelock -   Lock 140334994348296 acquired on ./drive/My Drive/discursos_reina/cached_lm_GPT2Tokenizer_1024_train.txt.lock\n",
            "10/10/2020 11:44:28 - INFO - filelock -   Lock 140334994348296 released on ./drive/My Drive/discursos_reina/cached_lm_GPT2Tokenizer_1024_train.txt.lock\n",
            "10/10/2020 11:44:28 - INFO - filelock -   Lock 140334994348072 acquired on ./drive/My Drive/discursos_reina/cached_lm_GPT2Tokenizer_1024_eval.txt.lock\n",
            "10/10/2020 11:44:28 - INFO - filelock -   Lock 140334994348072 released on ./drive/My Drive/discursos_reina/cached_lm_GPT2Tokenizer_1024_eval.txt.lock\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:267: FutureWarning: Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future version. Use `args.prediction_loss_only` instead.\n",
            "  FutureWarning,\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/106 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/106 [00:23<41:45, 23.86s/it]\u001b[A\n",
            "Iteration:   2% 2/106 [00:46<40:44, 23.50s/it]\u001b[A\n",
            "Iteration:   3% 3/106 [01:08<39:21, 22.92s/it]\u001b[A\n",
            "Iteration:   4% 4/106 [01:29<38:15, 22.51s/it]\u001b[A\n",
            "Iteration:   5% 5/106 [01:50<37:12, 22.11s/it]\u001b[A\n",
            "Iteration:   6% 6/106 [02:12<36:27, 21.87s/it]\u001b[A\n",
            "Iteration:   7% 7/106 [02:32<35:32, 21.54s/it]\u001b[A\n",
            "Iteration:   8% 8/106 [02:54<34:58, 21.41s/it]\u001b[A\n",
            "Iteration:   8% 9/106 [03:15<34:30, 21.35s/it]\u001b[A\n",
            "Iteration:   9% 10/106 [03:36<34:05, 21.31s/it]\u001b[A\n",
            "Iteration:  10% 11/106 [03:57<33:36, 21.22s/it]\u001b[A\n",
            "Iteration:  11% 12/106 [04:18<33:23, 21.32s/it]\u001b[A\n",
            "Iteration:  12% 13/106 [04:40<32:58, 21.27s/it]\u001b[A\n",
            "Iteration:  13% 14/106 [05:01<32:48, 21.39s/it]\u001b[A\n",
            "Iteration:  14% 15/106 [05:22<32:15, 21.27s/it]\u001b[A\n",
            "Iteration:  15% 16/106 [05:43<31:45, 21.18s/it]\u001b[A\n",
            "Iteration:  16% 17/106 [06:04<31:25, 21.19s/it]\u001b[A\n",
            "Iteration:  17% 18/106 [06:26<31:13, 21.29s/it]\u001b[A\n",
            "Iteration:  18% 19/106 [06:48<31:03, 21.42s/it]\u001b[A\n",
            "Iteration:  19% 20/106 [07:10<31:06, 21.70s/it]\u001b[A\n",
            "Iteration:  20% 21/106 [07:32<30:49, 21.75s/it]\u001b[A\n",
            "Iteration:  21% 22/106 [07:54<30:25, 21.73s/it]\u001b[A\n",
            "Iteration:  22% 23/106 [08:15<29:45, 21.51s/it]\u001b[A\n",
            "Iteration:  23% 24/106 [08:35<29:06, 21.30s/it]\u001b[A\n",
            "Iteration:  24% 25/106 [08:57<28:52, 21.38s/it]\u001b[A\n",
            "Iteration:  25% 26/106 [09:18<28:27, 21.35s/it]\u001b[A\n",
            "Iteration:  25% 27/106 [09:39<27:52, 21.18s/it]\u001b[A\n",
            "Iteration:  26% 28/106 [10:00<27:27, 21.12s/it]\u001b[A\n",
            "Iteration:  27% 29/106 [10:21<26:54, 20.97s/it]\u001b[A\n",
            "Iteration:  28% 30/106 [10:42<26:49, 21.18s/it]\u001b[A\n",
            "Iteration:  29% 31/106 [11:04<26:48, 21.44s/it]\u001b[A\n",
            "Iteration:  30% 32/106 [11:26<26:30, 21.50s/it]\u001b[A\n",
            "Iteration:  31% 33/106 [11:48<26:12, 21.54s/it]\u001b[A\n",
            "Iteration:  32% 34/106 [12:09<25:50, 21.53s/it]\u001b[A\n",
            "Iteration:  33% 35/106 [12:31<25:30, 21.56s/it]\u001b[A\n",
            "Iteration:  34% 36/106 [12:53<25:13, 21.62s/it]\u001b[A\n",
            "Iteration:  35% 37/106 [13:14<24:52, 21.64s/it]\u001b[A\n",
            "Iteration:  36% 38/106 [13:35<24:22, 21.51s/it]\u001b[A\n",
            "Iteration:  37% 39/106 [13:57<23:57, 21.46s/it]\u001b[A\n",
            "Iteration:  38% 40/106 [14:18<23:28, 21.34s/it]\u001b[A\n",
            "Iteration:  39% 41/106 [14:39<23:02, 21.28s/it]\u001b[A\n",
            "Iteration:  40% 42/106 [15:00<22:35, 21.18s/it]\u001b[A\n",
            "Iteration:  41% 43/106 [15:21<22:18, 21.25s/it]\u001b[A\n",
            "Iteration:  42% 44/106 [15:43<21:56, 21.23s/it]\u001b[A\n",
            "Iteration:  42% 45/106 [16:04<21:36, 21.25s/it]\u001b[A\n",
            "Iteration:  43% 46/106 [16:25<21:16, 21.27s/it]\u001b[A\n",
            "Iteration:  44% 47/106 [16:46<20:48, 21.15s/it]\u001b[A\n",
            "Iteration:  45% 48/106 [17:07<20:27, 21.16s/it]\u001b[A\n",
            "Iteration:  46% 49/106 [17:29<20:12, 21.28s/it]\u001b[A\n",
            "Iteration:  47% 50/106 [17:50<19:44, 21.15s/it]\u001b[A\n",
            "Iteration:  48% 51/106 [18:12<19:37, 21.41s/it]\u001b[A\n",
            "Iteration:  49% 52/106 [18:32<19:03, 21.17s/it]\u001b[A\n",
            "Iteration:  50% 53/106 [18:54<18:47, 21.27s/it]\u001b[A\n",
            "Iteration:  51% 54/106 [19:15<18:26, 21.28s/it]\u001b[A\n",
            "Iteration:  52% 55/106 [19:36<18:00, 21.19s/it]\u001b[A\n",
            "Iteration:  53% 56/106 [19:58<17:52, 21.45s/it]\u001b[A\n",
            "Iteration:  54% 57/106 [20:20<17:38, 21.59s/it]\u001b[A\n",
            "Iteration:  55% 58/106 [20:41<17:10, 21.46s/it]\u001b[A\n",
            "Iteration:  56% 59/106 [21:03<16:54, 21.58s/it]\u001b[A\n",
            "Iteration:  57% 60/106 [21:25<16:42, 21.79s/it]\u001b[A\n",
            "Iteration:  58% 61/106 [21:47<16:21, 21.81s/it]\u001b[A\n",
            "Iteration:  58% 62/106 [22:09<16:00, 21.84s/it]\u001b[A\n",
            "Iteration:  59% 63/106 [22:31<15:34, 21.73s/it]\u001b[A\n",
            "Iteration:  60% 64/106 [22:52<15:09, 21.66s/it]\u001b[A\n",
            "Iteration:  61% 65/106 [23:14<14:47, 21.64s/it]\u001b[A\n",
            "Iteration:  62% 66/106 [23:35<14:19, 21.49s/it]\u001b[A\n",
            "Iteration:  63% 67/106 [23:57<14:03, 21.62s/it]\u001b[A\n",
            "Iteration:  64% 68/106 [24:19<13:47, 21.77s/it]\u001b[A\n",
            "Iteration:  65% 69/106 [24:40<13:18, 21.59s/it]\u001b[A\n",
            "Iteration:  66% 70/106 [25:01<12:55, 21.54s/it]\u001b[A\n",
            "Iteration:  67% 71/106 [25:22<12:25, 21.30s/it]\u001b[A\n",
            "Iteration:  68% 72/106 [25:44<12:08, 21.42s/it]\u001b[A\n",
            "Iteration:  69% 73/106 [26:06<11:51, 21.57s/it]\u001b[A\n",
            "Iteration:  70% 74/106 [26:27<11:23, 21.36s/it]\u001b[A\n",
            "Iteration:  71% 75/106 [26:48<11:03, 21.41s/it]\u001b[A\n",
            "Iteration:  72% 76/106 [27:10<10:43, 21.45s/it]\u001b[A\n",
            "Iteration:  73% 77/106 [27:31<10:23, 21.49s/it]\u001b[A\n",
            "Iteration:  74% 78/106 [27:52<09:58, 21.39s/it]\u001b[A\n",
            "Iteration:  75% 79/106 [28:15<09:49, 21.83s/it]\u001b[A\n",
            "Iteration:  75% 80/106 [28:37<09:22, 21.65s/it]\u001b[A\n",
            "Iteration:  76% 81/106 [28:58<09:02, 21.69s/it]\u001b[A\n",
            "Iteration:  77% 82/106 [29:20<08:41, 21.73s/it]\u001b[A\n",
            "Iteration:  78% 83/106 [29:42<08:23, 21.88s/it]\u001b[A\n",
            "Iteration:  79% 84/106 [30:05<08:03, 21.99s/it]\u001b[A\n",
            "Iteration:  80% 85/106 [30:27<07:43, 22.07s/it]\u001b[A\n",
            "Iteration:  81% 86/106 [30:49<07:23, 22.17s/it]\u001b[A\n",
            "Iteration:  82% 87/106 [31:11<07:00, 22.15s/it]\u001b[A\n",
            "Iteration:  83% 88/106 [31:34<06:39, 22.20s/it]\u001b[A\n",
            "Iteration:  84% 89/106 [31:56<06:17, 22.22s/it]\u001b[A\n",
            "Iteration:  85% 90/106 [32:18<05:57, 22.32s/it]\u001b[A\n",
            "Iteration:  86% 91/106 [32:41<05:34, 22.30s/it]\u001b[A\n",
            "Iteration:  87% 92/106 [33:03<05:11, 22.26s/it]\u001b[A\n",
            "Iteration:  88% 93/106 [33:25<04:49, 22.29s/it]\u001b[A\n",
            "Iteration:  89% 94/106 [33:48<04:28, 22.34s/it]\u001b[A\n",
            "Iteration:  90% 95/106 [34:10<04:06, 22.37s/it]\u001b[A\n",
            "Iteration:  91% 96/106 [34:32<03:43, 22.33s/it]\u001b[A\n",
            "Iteration:  92% 97/106 [34:55<03:20, 22.32s/it]\u001b[A\n",
            "Iteration:  92% 98/106 [35:17<02:58, 22.35s/it]\u001b[A\n",
            "Iteration:  93% 99/106 [35:40<02:36, 22.38s/it]\u001b[A\n",
            "Iteration:  94% 100/106 [36:01<02:12, 22.16s/it]\u001b[A\n",
            "Iteration:  95% 101/106 [36:23<01:50, 22.02s/it]\u001b[A\n",
            "Iteration:  96% 102/106 [36:44<01:27, 21.80s/it]\u001b[A\n",
            "Iteration:  97% 103/106 [37:06<01:05, 21.69s/it]\u001b[A\n",
            "Iteration:  98% 104/106 [37:27<00:43, 21.68s/it]\u001b[A\n",
            "Iteration:  99% 105/106 [37:49<00:21, 21.70s/it]\u001b[A\n",
            "Iteration: 100% 106/106 [38:11<00:00, 21.62s/it]\n",
            "Epoch:  33% 1/3 [38:11<1:16:23, 2291.61s/it]\n",
            "Iteration:   0% 0/106 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/106 [00:22<39:53, 22.79s/it]\u001b[A\n",
            "Iteration:   2% 2/106 [00:45<39:20, 22.70s/it]\u001b[A\n",
            "Iteration:   3% 3/106 [01:07<38:33, 22.46s/it]\u001b[A\n",
            "Iteration:   4% 4/106 [01:29<38:02, 22.38s/it]\u001b[A\n",
            "Iteration:   5% 5/106 [01:50<37:08, 22.07s/it]\u001b[A\n",
            "Iteration:   6% 6/106 [02:13<36:58, 22.18s/it]\u001b[A\n",
            "Iteration:   7% 7/106 [02:35<36:41, 22.24s/it]\u001b[A\n",
            "Iteration:   8% 8/106 [02:57<36:15, 22.20s/it]\u001b[A\n",
            "Iteration:   8% 9/106 [03:19<35:53, 22.20s/it]\u001b[A\n",
            "Iteration:   9% 10/106 [03:42<35:33, 22.23s/it]\u001b[A\n",
            "Iteration:  10% 11/106 [04:04<35:14, 22.26s/it]\u001b[A\n",
            "Iteration:  11% 12/106 [04:26<34:55, 22.30s/it]\u001b[A\n",
            "Iteration:  12% 13/106 [04:49<34:38, 22.35s/it]\u001b[A\n",
            "Iteration:  13% 14/106 [05:11<34:18, 22.38s/it]\u001b[A\n",
            "Iteration:  14% 15/106 [05:34<33:58, 22.40s/it]\u001b[A\n",
            "Iteration:  15% 16/106 [05:56<33:31, 22.35s/it]\u001b[A\n",
            "Iteration:  16% 17/106 [06:18<33:11, 22.38s/it]\u001b[A\n",
            "Iteration:  17% 18/106 [06:41<32:49, 22.38s/it]\u001b[A\n",
            "Iteration:  18% 19/106 [07:03<32:32, 22.45s/it]\u001b[A\n",
            "Iteration:  19% 20/106 [07:26<32:15, 22.50s/it]\u001b[A\n",
            "Iteration:  20% 21/106 [07:49<31:54, 22.53s/it]\u001b[A\n",
            "Iteration:  21% 22/106 [08:11<31:29, 22.50s/it]\u001b[A\n",
            "Iteration:  22% 23/106 [08:33<31:05, 22.47s/it]\u001b[A\n",
            "Iteration:  23% 24/106 [08:55<30:24, 22.25s/it]\u001b[A\n",
            "Iteration:  24% 25/106 [09:17<29:50, 22.10s/it]\u001b[A\n",
            "Iteration:  25% 26/106 [09:39<29:24, 22.06s/it]\u001b[A\n",
            "Iteration:  25% 27/106 [10:01<29:05, 22.10s/it]\u001b[A\n",
            "Iteration:  26% 28/106 [10:24<29:02, 22.35s/it]\u001b[A\n",
            "Iteration:  27% 29/106 [10:46<28:36, 22.30s/it]\u001b[A\n",
            "Iteration:  28% 30/106 [11:08<28:13, 22.29s/it]\u001b[A\n",
            "Iteration:  29% 31/106 [11:31<27:46, 22.23s/it]\u001b[A\n",
            "Iteration:  30% 32/106 [11:53<27:22, 22.20s/it]\u001b[A\n",
            "Iteration:  31% 33/106 [12:14<26:49, 22.04s/it]\u001b[A\n",
            "Iteration:  32% 34/106 [12:35<26:01, 21.69s/it]\u001b[A\n",
            "Iteration:  33% 35/106 [12:57<25:39, 21.69s/it]\u001b[A\n",
            "Iteration:  34% 36/106 [13:18<25:15, 21.65s/it]\u001b[A\n",
            "Iteration:  35% 37/106 [13:41<25:09, 21.87s/it]\u001b[A\n",
            "Iteration:  36% 38/106 [14:03<25:01, 22.08s/it]\u001b[A\n",
            "Iteration:  37% 39/106 [14:26<24:47, 22.20s/it]\u001b[A\n",
            "Iteration:  38% 40/106 [14:48<24:31, 22.29s/it]\u001b[A\n",
            "Iteration:  39% 41/106 [15:11<24:13, 22.36s/it]\u001b[A\n",
            "Iteration:  40% 42/106 [15:34<23:55, 22.43s/it]\u001b[A\n",
            "Iteration:  41% 43/106 [15:56<23:32, 22.42s/it]\u001b[A\n",
            "Iteration:  42% 44/106 [16:19<23:16, 22.53s/it]\u001b[A\n",
            "Iteration:  42% 45/106 [16:41<22:50, 22.47s/it]\u001b[A\n",
            "Iteration:  43% 46/106 [17:03<22:20, 22.34s/it]\u001b[A\n",
            "Iteration:  44% 47/106 [17:25<21:52, 22.24s/it]\u001b[A\n",
            "Iteration:  45% 48/106 [17:47<21:22, 22.11s/it]\u001b[A\n",
            "Iteration:  46% 49/106 [18:09<21:01, 22.13s/it]\u001b[A\n",
            "Iteration:  47% 50/106 [18:31<20:35, 22.07s/it]\u001b[A\n",
            "Iteration:  48% 51/106 [18:53<20:11, 22.03s/it]\u001b[A\n",
            "Iteration:  49% 52/106 [19:15<19:54, 22.12s/it]\u001b[A\n",
            "Iteration:  50% 53/106 [19:38<19:39, 22.25s/it]\u001b[A\n",
            "Iteration:  51% 54/106 [20:00<19:11, 22.14s/it]\u001b[A\n",
            "Iteration:  52% 55/106 [20:22<18:52, 22.20s/it]\u001b[A\n",
            "Iteration:  53% 56/106 [20:45<18:40, 22.41s/it]\u001b[A\n",
            "Iteration:  54% 57/106 [21:07<18:17, 22.41s/it]\u001b[A\n",
            "Iteration:  55% 58/106 [21:30<17:53, 22.37s/it]\u001b[A\n",
            "Iteration:  56% 59/106 [21:51<17:22, 22.17s/it]\u001b[A\n",
            "Iteration:  57% 60/106 [22:12<16:40, 21.75s/it]\u001b[A\n",
            "Iteration:  58% 61/106 [22:33<16:02, 21.39s/it]\u001b[A\n",
            "Iteration:  58% 62/106 [22:53<15:29, 21.12s/it]\u001b[A\n",
            "Iteration:  59% 63/106 [23:14<15:03, 21.01s/it]\u001b[A\n",
            "Iteration:  60% 64/106 [23:35<14:38, 20.91s/it]\u001b[A\n",
            "Iteration:  61% 65/106 [23:55<14:12, 20.78s/it]\u001b[A\n",
            "Iteration:  62% 66/106 [24:16<13:49, 20.75s/it]\u001b[A\n",
            "Iteration:  63% 67/106 [24:36<13:27, 20.70s/it]\u001b[A\n",
            "Iteration:  64% 68/106 [24:57<13:05, 20.66s/it]\u001b[A\n",
            "Iteration:  65% 69/106 [25:18<12:44, 20.66s/it]\u001b[A\n",
            "Iteration:  66% 70/106 [25:38<12:24, 20.67s/it]\u001b[A\n",
            "Iteration:  67% 71/106 [25:59<12:02, 20.65s/it]\u001b[A\n",
            "Iteration:  68% 72/106 [26:21<11:54, 21.00s/it]\u001b[A\n",
            "Iteration:  69% 73/106 [26:43<11:46, 21.42s/it]\u001b[A\n",
            "Iteration:  70% 74/106 [27:05<11:35, 21.73s/it]\u001b[A\n",
            "Iteration:  71% 75/106 [27:28<11:19, 21.91s/it]\u001b[A\n",
            "Iteration:  72% 76/106 [27:50<10:55, 21.87s/it]\u001b[A\n",
            "Iteration:  73% 77/106 [28:12<10:37, 22.00s/it]\u001b[A\n",
            "Iteration:  74% 78/106 [28:34<10:15, 21.99s/it]\u001b[A\n",
            "Iteration:  75% 79/106 [28:56<09:53, 21.97s/it]\u001b[A\n",
            "Iteration:  75% 80/106 [29:18<09:36, 22.18s/it]\u001b[A\n",
            "Iteration:  76% 81/106 [29:41<09:14, 22.20s/it]\u001b[A\n",
            "Iteration:  77% 82/106 [30:03<08:52, 22.20s/it]\u001b[A\n",
            "Iteration:  78% 83/106 [30:26<08:33, 22.33s/it]\u001b[A\n",
            "Iteration:  79% 84/106 [30:48<08:12, 22.38s/it]\u001b[A\n",
            "Iteration:  80% 85/106 [31:10<07:49, 22.33s/it]\u001b[A\n",
            "Iteration:  81% 86/106 [31:32<07:23, 22.16s/it]\u001b[A\n",
            "Iteration:  82% 87/106 [31:54<06:59, 22.07s/it]\u001b[A\n",
            "Iteration:  83% 88/106 [32:16<06:38, 22.16s/it]\u001b[A\n",
            "Iteration:  84% 89/106 [32:38<06:16, 22.14s/it]\u001b[A\n",
            "Iteration:  85% 90/106 [33:01<05:54, 22.16s/it]\u001b[A\n",
            "Iteration:  86% 91/106 [33:23<05:33, 22.22s/it]\u001b[A\n",
            "Iteration:  87% 92/106 [33:45<05:08, 22.05s/it]\u001b[A\n",
            "Iteration:  88% 93/106 [34:06<04:44, 21.85s/it]\u001b[A\n",
            "Iteration:  89% 94/106 [34:29<04:25, 22.11s/it]\u001b[A\n",
            "Iteration:  90% 95/106 [34:50<04:02, 22.03s/it]\u001b[A\n",
            "Iteration:  91% 96/106 [35:13<03:41, 22.12s/it]\u001b[A\n",
            "Iteration:  92% 97/106 [35:34<03:17, 21.94s/it]\u001b[A\n",
            "Iteration:  92% 98/106 [35:57<02:56, 22.01s/it]\u001b[A\n",
            "Iteration:  93% 99/106 [36:19<02:34, 22.12s/it]\u001b[A\n",
            "Iteration:  94% 100/106 [36:41<02:12, 22.12s/it]\u001b[A\n",
            "Iteration:  95% 101/106 [37:03<01:50, 22.03s/it]\u001b[A\n",
            "Iteration:  96% 102/106 [37:25<01:27, 21.98s/it]\u001b[A\n",
            "Iteration:  97% 103/106 [37:46<01:05, 21.92s/it]\u001b[A\n",
            "Iteration:  98% 104/106 [38:09<00:44, 22.02s/it]\u001b[A\n",
            "Iteration:  99% 105/106 [38:31<00:21, 21.96s/it]\u001b[A\n",
            "Iteration: 100% 106/106 [38:53<00:00, 22.01s/it]\n",
            "Epoch:  67% 2/3 [1:17:04<38:24, 2304.04s/it]\n",
            "Iteration:   0% 0/106 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/106 [00:22<39:12, 22.40s/it]\u001b[A\n",
            "Iteration:   2% 2/106 [00:45<38:57, 22.48s/it]\u001b[A\n",
            "Iteration:   3% 3/106 [01:07<38:27, 22.40s/it]\u001b[A\n",
            "Iteration:   4% 4/106 [01:29<38:06, 22.41s/it]\u001b[A\n",
            "Iteration:   5% 5/106 [01:52<37:41, 22.39s/it]\u001b[A\n",
            "Iteration:   6% 6/106 [02:15<37:43, 22.63s/it]\u001b[A\n",
            "Iteration:   7% 7/106 [02:37<37:13, 22.56s/it]\u001b[A\n",
            "Iteration:   8% 8/106 [02:59<36:38, 22.43s/it]\u001b[A\n",
            "Iteration:   8% 9/106 [03:21<36:07, 22.35s/it]\u001b[A\n",
            "Iteration:   9% 10/106 [03:44<35:51, 22.41s/it]\u001b[A\n",
            "Iteration:  10% 11/106 [04:06<35:28, 22.40s/it]\u001b[A\n",
            "Iteration:  11% 12/106 [04:29<35:09, 22.44s/it]\u001b[A\n",
            "Iteration:  12% 13/106 [04:51<34:38, 22.35s/it]\u001b[A\n",
            "Iteration:  13% 14/106 [05:13<33:55, 22.13s/it]\u001b[A\n",
            "Iteration:  14% 15/106 [05:34<33:08, 21.85s/it]\u001b[A\n",
            "Iteration:  15% 16/106 [05:55<32:37, 21.75s/it]\u001b[A\n",
            "Iteration:  16% 17/106 [06:17<32:10, 21.69s/it]\u001b[A\n",
            "Iteration:  17% 18/106 [06:39<31:59, 21.82s/it]\u001b[A\n",
            "Iteration:  18% 19/106 [07:01<31:35, 21.79s/it]\u001b[A\n",
            "Iteration:  19% 20/106 [07:24<31:42, 22.12s/it]\u001b[A\n",
            "Iteration:  20% 21/106 [07:46<31:28, 22.22s/it]\u001b[A\n",
            "Iteration:  21% 22/106 [08:08<30:57, 22.11s/it]\u001b[A\n",
            "Iteration:  22% 23/106 [08:30<30:34, 22.11s/it]\u001b[A\n",
            "Iteration:  23% 24/106 [08:52<30:15, 22.13s/it]\u001b[A\n",
            "Iteration:  24% 25/106 [09:15<29:56, 22.18s/it]\u001b[A\n",
            "Iteration:  25% 26/106 [09:36<29:17, 21.97s/it]\u001b[A\n",
            "Iteration:  25% 27/106 [09:57<28:34, 21.71s/it]\u001b[A\n",
            "Iteration:  26% 28/106 [10:19<28:08, 21.64s/it]\u001b[A\n",
            "Iteration:  27% 29/106 [10:40<27:50, 21.70s/it]\u001b[A\n",
            "Iteration:  28% 30/106 [11:02<27:21, 21.59s/it]\u001b[A\n",
            "Iteration:  29% 31/106 [11:24<27:16, 21.82s/it]\u001b[A\n",
            "Iteration:  30% 32/106 [11:47<27:12, 22.06s/it]\u001b[A\n",
            "Iteration:  31% 33/106 [12:10<27:13, 22.38s/it]\u001b[A\n",
            "Iteration:  32% 34/106 [12:33<27:01, 22.52s/it]\u001b[A\n",
            "Iteration:  33% 35/106 [12:54<26:19, 22.24s/it]\u001b[A\n",
            "Iteration:  34% 36/106 [13:15<25:34, 21.93s/it]\u001b[A\n",
            "Iteration:  35% 37/106 [13:37<25:04, 21.80s/it]\u001b[A\n",
            "Iteration:  36% 38/106 [13:58<24:31, 21.65s/it]\u001b[A\n",
            "Iteration:  37% 39/106 [14:21<24:26, 21.88s/it]\u001b[A\n",
            "Iteration:  38% 40/106 [14:43<24:08, 21.94s/it]\u001b[A\n",
            "Iteration:  39% 41/106 [15:05<23:42, 21.88s/it]\u001b[A\n",
            "Iteration:  40% 42/106 [15:26<23:13, 21.78s/it]\u001b[A\n",
            "Iteration:  41% 43/106 [15:48<22:49, 21.73s/it]\u001b[A\n",
            "Iteration:  42% 44/106 [16:10<22:32, 21.82s/it]\u001b[A\n",
            "Iteration:  42% 45/106 [16:32<22:18, 21.94s/it]\u001b[A\n",
            "Iteration:  43% 46/106 [16:54<21:54, 21.91s/it]\u001b[A\n",
            "Iteration:  44% 47/106 [17:16<21:35, 21.96s/it]\u001b[A\n",
            "Iteration:  45% 48/106 [17:38<21:18, 22.05s/it]\u001b[A\n",
            "Iteration:  46% 49/106 [18:01<21:02, 22.15s/it]\u001b[A\n",
            "Iteration:  47% 50/106 [18:23<20:43, 22.21s/it]\u001b[A\n",
            "Iteration:  48% 51/106 [18:45<20:21, 22.22s/it]\u001b[A\n",
            "Iteration:  49% 52/106 [19:07<19:54, 22.13s/it]\u001b[A\n",
            "Iteration:  50% 53/106 [19:30<19:39, 22.25s/it]\u001b[A\n",
            "Iteration:  51% 54/106 [19:52<19:18, 22.28s/it]\u001b[A\n",
            "Iteration:  52% 55/106 [20:14<18:58, 22.32s/it]\u001b[A\n",
            "Iteration:  53% 56/106 [20:37<18:38, 22.37s/it]\u001b[A\n",
            "Iteration:  54% 57/106 [20:59<18:14, 22.33s/it]\u001b[A\n",
            "Iteration:  55% 58/106 [21:21<17:51, 22.33s/it]\u001b[A\n",
            "Iteration:  56% 59/106 [21:44<17:32, 22.39s/it]\u001b[A\n",
            "Iteration:  57% 60/106 [22:06<17:11, 22.43s/it]\u001b[A\n",
            "Iteration:  58% 61/106 [22:30<17:03, 22.75s/it]\u001b[A\n",
            "Iteration:  58% 62/106 [22:52<16:36, 22.65s/it]\u001b[A\n",
            "Iteration:  59% 63/106 [23:15<16:13, 22.64s/it]\u001b[A\n",
            "Iteration:  60% 64/106 [23:38<15:50, 22.63s/it]\u001b[A\n",
            "Iteration:  61% 65/106 [23:59<15:18, 22.41s/it]\u001b[A\n",
            "Iteration:  62% 66/106 [24:21<14:47, 22.18s/it]\u001b[A\n",
            "Iteration:  63% 67/106 [24:43<14:26, 22.21s/it]\u001b[A\n",
            "Iteration:  64% 68/106 [25:05<14:00, 22.12s/it]\u001b[A\n",
            "Iteration:  65% 69/106 [25:27<13:35, 22.03s/it]\u001b[A\n",
            "Iteration:  66% 70/106 [25:49<13:11, 21.99s/it]\u001b[A\n",
            "Iteration:  67% 71/106 [26:11<12:49, 21.97s/it]\u001b[A\n",
            "Iteration:  68% 72/106 [26:33<12:32, 22.14s/it]\u001b[A\n",
            "Iteration:  69% 73/106 [26:56<12:10, 22.13s/it]\u001b[A\n",
            "Iteration:  70% 74/106 [27:18<11:50, 22.19s/it]\u001b[A\n",
            "Iteration:  71% 75/106 [27:41<11:31, 22.31s/it]\u001b[A\n",
            "Iteration:  72% 76/106 [28:03<11:10, 22.36s/it]\u001b[A\n",
            "Iteration:  73% 77/106 [28:25<10:49, 22.39s/it]\u001b[A\n",
            "Iteration:  74% 78/106 [28:48<10:28, 22.46s/it]\u001b[A\n",
            "Iteration:  75% 79/106 [29:11<10:06, 22.46s/it]\u001b[A\n",
            "Iteration:  75% 80/106 [29:33<09:44, 22.49s/it]\u001b[A\n",
            "Iteration:  76% 81/106 [29:55<09:20, 22.41s/it]\u001b[A\n",
            "Iteration:  77% 82/106 [30:17<08:54, 22.26s/it]\u001b[A\n",
            "Iteration:  78% 83/106 [30:39<08:30, 22.22s/it]\u001b[A\n",
            "Iteration:  79% 84/106 [31:01<08:07, 22.15s/it]\u001b[A\n",
            "Iteration:  80% 85/106 [31:23<07:44, 22.10s/it]\u001b[A\n",
            "Iteration:  81% 86/106 [31:46<07:23, 22.15s/it]\u001b[A\n",
            "Iteration:  82% 87/106 [32:08<07:01, 22.20s/it]\u001b[A\n",
            "Iteration:  83% 88/106 [32:31<06:42, 22.38s/it]\u001b[A\n",
            "Iteration:  84% 89/106 [32:52<06:17, 22.20s/it]\u001b[A\n",
            "Iteration:  85% 90/106 [33:15<05:56, 22.30s/it]\u001b[A\n",
            "Iteration:  86% 91/106 [33:38<05:35, 22.37s/it]\u001b[A\n",
            "Iteration:  87% 92/106 [34:00<05:11, 22.26s/it]\u001b[A\n",
            "Iteration:  88% 93/106 [34:22<04:48, 22.21s/it]\u001b[A\n",
            "Iteration:  89% 94/106 [34:43<04:24, 22.08s/it]\u001b[A\n",
            "Iteration:  90% 95/106 [35:06<04:03, 22.10s/it]\u001b[A\n",
            "Iteration:  91% 96/106 [35:27<03:40, 22.01s/it]\u001b[A\n",
            "Iteration:  92% 97/106 [35:49<03:17, 21.98s/it]\u001b[A\n",
            "Iteration:  92% 98/106 [36:12<02:56, 22.07s/it]\u001b[A\n",
            "Iteration:  93% 99/106 [36:34<02:34, 22.13s/it]\u001b[A\n",
            "Iteration:  94% 100/106 [36:56<02:12, 22.15s/it]\u001b[A\n",
            "Iteration:  95% 101/106 [37:18<01:50, 22.18s/it]\u001b[A\n",
            "Iteration:  96% 102/106 [37:41<01:29, 22.26s/it]\u001b[A\n",
            "Iteration:  97% 103/106 [38:02<01:06, 22.04s/it]\u001b[A\n",
            "Iteration:  98% 104/106 [38:24<00:43, 21.82s/it]\u001b[A\n",
            "Iteration:  99% 105/106 [38:45<00:21, 21.69s/it]\u001b[A\n",
            "Iteration: 100% 106/106 [39:07<00:00, 22.15s/it]\n",
            "Epoch: 100% 3/3 [1:56:12<00:00, 2324.08s/it]\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:1175: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n",
            "  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n",
            "10/10/2020 13:40:43 - INFO - __main__ -   *** Evaluate ***\n",
            "Evaluation: 100% 50/50 [05:17<00:00,  6.35s/it]\n",
            "{'eval_loss': 3.0327692937850954, 'epoch': 3.0, 'total_flos': 243129501351936, 'step': 318}\n",
            "10/10/2020 13:46:01 - INFO - __main__ -   ***** Eval results *****\n",
            "10/10/2020 13:46:01 - INFO - __main__ -     perplexity = 20.75462874369381\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYJwyq8m01BU",
        "outputId": "9d2d47b4-ad77-46d1-8d6c-d90cb9a9daa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "# Fine-tuning del modelo con nuestros datos\n",
        "!python3 ./drive/My\\ Drive/run_language_modeling.py \\\n",
        "  --output_dir='./drive/My Drive/fine_tuning_bert' \\\n",
        "  --overwrite_output_dir \\\n",
        "  --model_type=bert \\\n",
        "  --mlm \\\n",
        "  --model_name_or_path=bert-base-cased \\\n",
        "  --do_train \\\n",
        "  --train_data_file='./drive/My Drive/discursos_reina/train.txt' \\\n",
        "  --do_eval \\\n",
        "  --eval_data_file='./drive/My Drive/discursos_reina/eval.txt' \\\n",
        "  --per_device_train_batch_size=1 \\\n",
        "  --per_device_eval_batch_size=1 \\\n",
        "  --evaluate_during_training \\\n",
        "  --learning_rate=5e-5 \\\n",
        "  --num_train_epochs=3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-26 16:55:32.324584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
            "  FutureWarning,\n",
            "10/26/2020 16:55:34 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "10/26/2020 16:55:34 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='./drive/My Drive/fine_tuning_bert', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Oct26_16-55-34_7cf5294f04ca', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='./drive/My Drive/fine_tuning_bert', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_auto.py:825: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1374: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
            "  FutureWarning,\n",
            "10/26/2020 16:55:39 - INFO - filelock -   Lock 140387993118536 acquired on ./drive/My Drive/discursos_reina/cached_lm_BertTokenizer_510_train.txt.lock\n",
            "10/26/2020 16:55:41 - INFO - filelock -   Lock 140387993118536 released on ./drive/My Drive/discursos_reina/cached_lm_BertTokenizer_510_train.txt.lock\n",
            "10/26/2020 16:55:41 - INFO - filelock -   Lock 140387992795624 acquired on ./drive/My Drive/discursos_reina/cached_lm_BertTokenizer_510_eval.txt.lock\n",
            "10/26/2020 16:55:43 - INFO - filelock -   Lock 140387992795624 released on ./drive/My Drive/discursos_reina/cached_lm_BertTokenizer_510_eval.txt.lock\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:263: FutureWarning: Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future version. Use `args.prediction_loss_only` instead. Setting `args.prediction_loss_only=True\n",
            "  FutureWarning,\n",
            "  5% 32/636 [04:16<1:23:04,  8.25s/it]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_FdfmMm1zLN"
      },
      "source": [
        "# Fine-tuning del modelo con nuestros datos\n",
        "!python3 ./drive/My\\ Drive/run_language_modeling.py \\\n",
        "  --output_dir='./drive/My Drive/fine_tuning_bert_large' \\\n",
        "  --overwrite_output_dir \\\n",
        "  --model_type=bert \\\n",
        "  --mlm \\\n",
        "  --model_name_or_path=bert-large-cased \\\n",
        "  --do_train \\\n",
        "  --train_data_file='./drive/My Drive/discursos_reina/train.txt' \\\n",
        "  --do_eval \\\n",
        "  --eval_data_file='./drive/My Drive/discursos_reina/eval.txt' \\\n",
        "  --per_device_train_batch_size=1 \\\n",
        "  --per_device_eval_batch_size=1 \\\n",
        "  --evaluate_during_training \\\n",
        "  --learning_rate=5e-5 \\\n",
        "  --num_train_epochs=3"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}